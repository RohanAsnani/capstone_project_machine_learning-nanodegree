{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9K6JKJ4xt6ds"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "id": "Lhc8rolWz1Ej",
    "outputId": "7028a259-6793-429d-a740-e1832b7fc890"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4497778049515494309\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ObBwxPRzz32N"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Add,Concatenate, Dropout,Conv2D\n",
    "from keras.models import Model\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 100\n",
    "#VALIDATION_SPLIT = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MmSDSRf-z_0C"
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Cleaning of dataset\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)    \n",
    "    string = re.sub(r\"\\'\", \"\", string)    \n",
    "    string = re.sub(r\"\\\"\", \"\", string)    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.311660e+17</td>\n",
       "      <td>ðŸ”¥ca kkk grand wizard ðŸ”¥ endorses @hillary...</td>\n",
       "      <td>unverified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.145990e+17</td>\n",
       "      <td>an open letter to trump voters from his top st...</td>\n",
       "      <td>unverified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.918090e+17</td>\n",
       "      <td>america is a nation of second chances â€”@potu...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.932050e+17</td>\n",
       "      <td>brandon marshall visits and offers advice, sup...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.511000e+17</td>\n",
       "      <td>rip elly may clampett: so sad to learn #beverl...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.672240e+17</td>\n",
       "      <td>former 3 doors down guitarist matt roberts has...</td>\n",
       "      <td>non-rumor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.155160e+17</td>\n",
       "      <td>craigslist ad: â€˜get paid $15 an hour to prot...</td>\n",
       "      <td>unverified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.141060e+17</td>\n",
       "      <td>just in: missing afghan soldiers found trying ...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.003200e+17</td>\n",
       "      <td>the day #ferguson cops told a dirty, bloody li...</td>\n",
       "      <td>unverified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.003200e+17</td>\n",
       "      <td>the day #ferguson cops told a dirty, bloody li...</td>\n",
       "      <td>unverified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.003200e+17</td>\n",
       "      <td>if we shot everyone in the street for stealing...</td>\n",
       "      <td>unverified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.003200e+17</td>\n",
       "      <td>if we shot everyone in the street for stealing...</td>\n",
       "      <td>unverified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.953670e+17</td>\n",
       "      <td>#riphulkhogan my heart is ripping like your sh...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.322070e+17</td>\n",
       "      <td>a chick-fil-a manager allegedly banned this hi...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.601880e+17</td>\n",
       "      <td>islamic tribunal using sharia law in texas has...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.315690e+17</td>\n",
       "      <td>alpha male rt @marc_leibowitz: photo of vladim...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.898290e+17</td>\n",
       "      <td>à¹€à¸œà¸¢à¸à¸šà¸à¸¢à¸¹à¹€à¸„à¸£à¸™à¸­à¸²à¸ˆà...</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.249260e+17</td>\n",
       "      <td>shooter still on loose after uniformed soldier...</td>\n",
       "      <td>unverified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.249260e+17</td>\n",
       "      <td>shooter still on loose after uniformed soldier...</td>\n",
       "      <td>unverified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.249260e+17</td>\n",
       "      <td>shooter still on loose after uniformed soldier...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                               text  \\\n",
       "0   7.311660e+17  ðŸ”¥ca kkk grand wizard ðŸ”¥ endorses @hillary...   \n",
       "1   7.145990e+17  an open letter to trump voters from his top st...   \n",
       "2   6.918090e+17  america is a nation of second chances â€”@potu...   \n",
       "3   6.932050e+17  brandon marshall visits and offers advice, sup...   \n",
       "4   5.511000e+17  rip elly may clampett: so sad to learn #beverl...   \n",
       "5   7.672240e+17  former 3 doors down guitarist matt roberts has...   \n",
       "6   7.155160e+17  craigslist ad: â€˜get paid $15 an hour to prot...   \n",
       "7   5.141060e+17  just in: missing afghan soldiers found trying ...   \n",
       "8   5.003200e+17  the day #ferguson cops told a dirty, bloody li...   \n",
       "9   5.003200e+17  the day #ferguson cops told a dirty, bloody li...   \n",
       "10  5.003200e+17  if we shot everyone in the street for stealing...   \n",
       "11  5.003200e+17  if we shot everyone in the street for stealing...   \n",
       "12  4.953670e+17  #riphulkhogan my heart is ripping like your sh...   \n",
       "13  5.322070e+17  a chick-fil-a manager allegedly banned this hi...   \n",
       "14  5.601880e+17  islamic tribunal using sharia law in texas has...   \n",
       "15  5.315690e+17  alpha male rt @marc_leibowitz: photo of vladim...   \n",
       "16  4.898290e+17  à¹€à¸œà¸¢à¸à¸šà¸à¸¢à¸¹à¹€à¸„à¸£à¸™à¸­à¸²à¸ˆà...   \n",
       "17  5.249260e+17  shooter still on loose after uniformed soldier...   \n",
       "18  5.249260e+17  shooter still on loose after uniformed soldier...   \n",
       "19  5.249260e+17  shooter still on loose after uniformed soldier...   \n",
       "\n",
       "         label  \n",
       "0   unverified  \n",
       "1   unverified  \n",
       "2    non-rumor  \n",
       "3    non-rumor  \n",
       "4         true  \n",
       "5    non-rumor  \n",
       "6   unverified  \n",
       "7         true  \n",
       "8   unverified  \n",
       "9   unverified  \n",
       "10  unverified  \n",
       "11  unverified  \n",
       "12       false  \n",
       "13       false  \n",
       "14       false  \n",
       "15       false  \n",
       "16       false  \n",
       "17  unverified  \n",
       "18  unverified  \n",
       "19        true  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.chdir(\"C:\\\\Users\\\\rohan\\\\Desktop\\\\LeadingIndia.ai\\\\TASK3\")\n",
    "df1= pd.read_csv('pt1.csv')\n",
    "df2= pd.read_csv('pt2.csv')\n",
    "#data_valid = pd.read_csv(\"valid.tsv\", sep='\\t')\n",
    "#data_test=pd.read_csv(\"test.tsv\",sep='\\t')\n",
    "#data_train.head()\n",
    "\n",
    "#data_train.columns = ['id','label','statement','subject','speaker', 'job', 'state','party','barely_true_c','false_c','half_true_c','mostly_true_c','pants_on_fire_c','venue']\n",
    " \n",
    "#data_valid.columns =['id','label','statement','subject','speaker', 'job', 'state','party','barely_true_c','false_c','half_true_c','mostly_true_c','pants_on_fire_c','venue']\n",
    "\n",
    "\n",
    "\n",
    "#data_test = ['id','label','statement','subject','speaker', 'job', 'state','party','barely_true_c','false_c','half_true_c','mostly_true_c','pants_on_fire_c','venue']\n",
    "\n",
    "\n",
    "#df = pd.concat([data_train, data_valid])\n",
    "\n",
    "data_train=df2.merge(df1, left_on='id', right_on='id')\n",
    "data_train.head(5)\n",
    "data_train.to_csv('data.csv')\n",
    "#data_train.columns\n",
    "#data_train['Label']\n",
    "#data_train['Label']\n",
    "\n",
    "\n",
    "#data_train=pd.read_csv(\"data.csv\")\n",
    "#data_train[\"Label\"]\n",
    "data_train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "id": "ptSS_YQM16-3",
    "outputId": "fbe26795-79dd-4d2a-c7fc-87803fcf11ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'text', 'label'], dtype='object')\n",
      "What the raw input data looks like:\n",
      "             id                                               text       label\n",
      "0  7.311660e+17  ðŸ”¥ca kkk grand wizard ðŸ”¥ endorses @hillary...  unverified\n",
      "1  7.145990e+17  an open letter to trump voters from his top st...  unverified\n",
      "2  6.918090e+17  america is a nation of second chances â€”@potu...   non-rumor\n",
      "3  6.932050e+17  brandon marshall visits and offers advice, sup...   non-rumor\n",
      "4  5.511000e+17  rip elly may clampett: so sad to learn #beverl...        true\n",
      "(1714, 3)\n",
      "1714\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Input Data preprocessing\n",
    "\n",
    "print(data_train.columns)\n",
    "print('What the raw input data looks like:')\n",
    "print(data_train[0:5])\n",
    "print(data_train.shape)\n",
    "#data_train = data_train.drop(['Unnamed: 0'],axis = 1)\n",
    "print(data_train.shape[0])\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for i in range(data_train.shape[0]):\n",
    "    text1 = data_train.id[i]\n",
    "    text2 = data_train.text[i]\n",
    "    \n",
    "    text = str(text1) +\"\"+ str(text2) \n",
    "    texts.append(text)\n",
    "    if data_train.label[i] == 'false' :\n",
    "        labels.append(0)\n",
    "    elif data_train.label[i] == 'true' :\n",
    "        labels.append(1)\n",
    "    elif data_train.label[i] == 'unverified' : \n",
    "        labels.append(2)\n",
    "    elif data_train.label[i] == 'non-rumor' : \n",
    "        labels.append(3)\n",
    "    \n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 3, 3, 1, 3, 2, 1, 2, 2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 1, 2, 2, 1, 2, 2, 1, 3, 2, 0, 3, 1, 0, 3, 2, 0, 2, 2, 2, 0, 3, 0, 0, 3, 1, 1, 1, 1, 0, 2, 3, 2, 0, 3, 3, 3, 3, 2, 2, 2, 2, 2, 0, 2, 2, 3, 2, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 2, 2, 2, 3, 3, 0, 1, 1, 1, 1, 0, 2, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 1, 1, 2, 3, 1, 0, 1, 2, 1, 1, 2, 3, 3, 0, 2, 2, 3, 1, 1, 3, 0, 2, 2, 3, 0, 0, 3, 3, 2, 2, 2, 2, 3, 0, 3, 0, 3, 2, 2, 0, 1, 2, 2, 1, 1, 3, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 2, 3, 3, 1, 3, 2, 2, 1, 1, 1, 1, 0, 0, 2, 2, 3, 1, 1, 1, 1, 1, 3, 0, 3, 3, 3, 1, 1, 1, 1, 2, 3, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 1, 0, 0, 1, 0, 1, 2, 1, 1, 1, 0, 2, 2, 2, 3, 1, 2, 1, 2, 3, 3, 1, 2, 1, 1, 1, 2, 1, 0, 2, 1, 2, 3, 1, 1, 1, 1, 0, 2, 0, 1, 3, 1, 1, 0, 0, 0, 2, 3, 0, 0, 3, 0, 3, 3, 3, 3, 3, 0, 1, 2, 2, 2, 2, 3, 3, 2, 2, 1, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 3, 2, 3, 3, 1, 0, 0, 2, 0, 2, 0, 3, 0, 0, 1, 0, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 0, 2, 2, 0, 3, 0, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 0, 3, 2, 3, 1, 2, 2, 3, 2, 2, 2, 1, 2, 2, 0, 1, 1, 1, 1, 0, 3, 2, 1, 3, 2, 3, 0, 0, 2, 1, 1, 1, 0, 3, 0, 3, 3, 3, 3, 3, 0, 3, 2, 2, 3, 0, 1, 2, 3, 3, 3, 3, 3, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 3, 1, 1, 3, 1, 0, 2, 0, 0, 0, 2, 0, 3, 2, 1, 1, 0, 1, 3, 2, 1, 0, 2, 1, 1, 1, 1, 1, 3, 0, 2, 2, 3, 0, 1, 0, 1, 1, 1, 1, 1, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 1, 2, 2, 2, 2, 1, 3, 2, 0, 2, 0, 0, 2, 0, 3, 0, 3, 1, 3, 3, 3, 2, 1, 2, 2, 0, 2, 0, 2, 2, 2, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 2, 3, 0, 0, 0, 0, 3, 3, 0, 2, 0, 3, 1, 1, 2, 2, 2, 3, 3, 3, 2, 3, 1, 0, 0, 0, 2, 2, 3, 1, 2, 1, 2, 0, 0, 2, 3, 0, 2, 3, 3, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 3, 2, 0, 3, 0, 0, 3, 0, 1, 1, 0, 1, 1, 1, 1, 3, 0, 0, 3, 1, 3, 1, 3, 3, 2, 2, 2, 0, 2, 3, 1, 1, 1, 1, 1, 0, 2, 3, 0, 3, 1, 0, 2, 2, 1, 1, 1, 1, 2, 3, 1, 0, 0, 3, 1, 3, 2, 1, 1, 2, 2, 2, 1, 0, 3, 3, 1, 0, 3, 1, 0, 2, 1, 1, 2, 1, 0, 0, 2, 3, 0, 1, 1, 1, 1, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 3, 1, 1, 1, 1, 1, 3, 1, 0, 2, 1, 3, 3, 0, 0, 0, 2, 3, 3, 0, 3, 3, 1, 3, 2, 0, 2, 1, 3, 0, 3, 2, 3, 1, 1, 0, 1, 2, 0, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 1, 3, 3, 0, 3, 1, 2, 3, 1, 0, 0, 2, 3, 0, 3, 1, 2, 3, 0, 0, 1, 1, 3, 2, 2, 0, 2, 3, 2, 0, 0, 2, 3, 3, 2, 0, 1, 3, 3, 2, 0, 0, 1, 3, 2, 1, 1, 1, 1, 3, 0, 1, 1, 1, 1, 1, 3, 0, 1, 1, 3, 3, 3, 3, 1, 1, 3, 0, 2, 1, 1, 2, 2, 2, 3, 0, 1, 2, 3, 1, 1, 1, 1, 1, 3, 1, 2, 3, 3, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 3, 2, 0, 1, 3, 3, 1, 1, 2, 1, 2, 3, 3, 2, 3, 1, 2, 3, 1, 2, 2, 3, 1, 3, 1, 1, 2, 2, 1, 2, 2, 3, 1, 1, 0, 3, 0, 2, 3, 0, 1, 3, 3, 1, 3, 3, 2, 1, 3, 2, 2, 3, 1, 3, 2, 3, 3, 3, 3, 2, 1, 3, 3, 3, 3, 1, 1, 1, 2, 1, 0, 2, 1, 0, 0, 3, 2, 2, 3, 0, 1, 2, 2, 0, 3, 0, 0, 0, 2, 2, 2, 2, 0, 3, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0, 2, 2, 2, 0, 2, 3, 1, 2, 2, 3, 0, 0, 2, 1, 2, 2, 1, 0, 1, 1, 1, 3, 1, 0, 0, 3, 3, 0, 0, 3, 3, 3, 0, 0, 3, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 2, 0, 0, 3, 1, 1, 2, 0, 1, 3, 0, 2, 1, 2, 3, 2, 3, 2, 3, 2, 1, 1, 3, 1, 3, 0, 3, 2, 2, 1, 0, 2, 2, 3, 1, 1, 1, 3, 0, 2, 3, 1, 0, 3, 3, 0, 3, 3, 3, 0, 1, 1, 3, 3, 0, 1, 1, 1, 3, 3, 1, 2, 3, 1, 0, 3, 3, 1, 1, 0, 0, 0, 1, 2, 0, 0, 3, 3, 3, 0, 0, 3, 2, 2, 2, 1, 2, 3, 3, 1, 0, 0, 0, 0, 1, 3, 3, 3, 3, 2, 2, 1, 3, 2, 1, 3, 0, 0, 2, 2, 3, 3, 2, 2, 3, 2, 1, 1, 1, 2, 1, 3, 1, 0, 1, 2, 2, 1, 2, 2, 2, 0, 1, 3, 0, 0, 1, 2, 3, 0, 3, 3, 0, 2, 3, 3, 1, 0, 3, 2, 0, 3, 0, 0, 1, 0, 1, 0, 2, 0, 3, 3, 2, 2, 0, 0, 3, 3, 0, 0, 2, 1, 1, 0, 3, 0, 0, 3, 2, 2, 3, 2, 3, 3, 2, 1, 2, 1, 1, 3, 3, 1, 1, 3, 2, 3, 1, 0, 3, 0, 1, 3, 1, 2, 1, 2, 3, 1, 1, 2, 3, 1, 3, 1, 2, 2, 2, 2, 0, 0, 0, 0, 3, 2, 3, 0, 3, 0, 0, 1, 3, 1, 1, 2, 2, 2, 1, 0, 3, 0, 2, 0, 0, 0, 1, 1, 2, 3, 0, 3, 3, 2, 0, 1, 0, 1, 2, 2, 3, 0, 0, 2, 2, 3, 0, 0, 2, 3, 1, 0, 3, 3, 3, 3, 1, 0, 3, 2, 2, 0, 3, 3, 0, 0, 3, 2, 2, 2, 0, 0, 0, 0, 2, 0, 0, 1, 2, 3, 3, 3, 1, 0, 2, 0, 0, 0, 2, 2, 0, 1, 1, 0, 2, 0, 2, 0, 2, 3, 2, 0, 2, 0, 1, 3, 3, 0, 0, 3, 2, 3, 3, 3, 0, 3, 0, 0, 0, 1, 2, 0, 0, 3, 3, 0, 3, 2, 3, 2, 1, 3, 3, 3, 3, 3, 1, 1, 0, 0, 2, 3, 3, 2, 2, 2, 0, 1, 0, 2, 1, 1, 3, 3, 0, 0, 0, 0, 1, 1, 2, 2, 0, 0, 2, 2, 2, 3, 3, 2, 3, 2, 1, 3, 2, 2, 0, 3, 1, 2, 1, 3, 3, 2, 0, 3, 2, 0, 0, 1, 2, 1, 2, 0, 0, 2, 3, 1, 0, 0, 2, 0, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 0, 1, 2, 0, 2, 2, 0, 2, 1, 2, 3, 2, 1, 1, 1, 2, 0, 1, 1, 2, 2, 1, 2, 0, 1, 1, 2, 0, 0, 3, 3, 2, 0, 2, 0, 2, 3, 3, 1, 2, 0, 2, 3, 2, 3, 0, 3, 3, 3, 2, 3, 3, 0, 0, 2, 0, 2, 0, 3, 0, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 3, 0, 0, 3, 2, 2, 3, 2, 1, 2, 1, 0, 0, 3, 2, 1, 2, 0, 0, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 3, 3, 0, 0, 2, 0, 0, 0, 2, 1, 2, 0, 3, 3, 3, 2, 1, 3, 1, 0, 3, 0, 1, 1, 2, 2, 2, 2, 2, 2, 0, 1, 3, 2, 0, 1, 1, 1, 3, 0, 3, 3, 1, 0, 2, 1, 2, 1, 1, 0, 1, 2, 3, 1, 0, 1, 0, 1, 0, 3, 0, 0, 0, 0, 0, 3, 3, 3, 2, 2, 2, 0, 1, 0, 3, 2, 0, 2, 2, 0, 2, 2, 3, 2, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2, 3, 2, 3, 1, 1, 0, 0, 2, 2, 3, 0, 0, 1, 1, 3, 2, 0, 2, 3, 3, 3, 3, 3, 0, 0, 3, 1, 0, 3, 2, 2, 2, 1, 1, 3, 0, 3, 1, 3, 1, 1, 3, 1, 3, 2, 0, 2]\n",
      "1714\n",
      "Found 6996 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "print(len(labels))\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "3Zl40bqu29IP",
    "outputId": "aa50ab04-92a5-49c7-bf72-63d2c072aa0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (1714, 100)\n",
      "Shape of label tensor: (1714, 6, 4)\n"
     ]
    }
   ],
   "source": [
    "# Pad input sequences\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(labels),num_classes = 4)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "oqBPGbZrURJS",
    "outputId": "fdb94202-85b6-4d3b-873e-f448724f1038"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train, validation, test: 1628 1029 685\n",
      "real & fake news in train,valt,test:\n",
      "[[1247.  381.    0.    0.]\n",
      " [1141.  487.    0.    0.]\n",
      " [1250.  378.    0.    0.]\n",
      " [1246.  382.    0.    0.]\n",
      " [1628.    0.    0.    0.]\n",
      " [1628.    0.    0.    0.]]\n",
      "[[ 791.  238.    0.    0.]\n",
      " [ 717.  312.    0.    0.]\n",
      " [ 796.  233.    0.    0.]\n",
      " [ 783.  246.    0.    0.]\n",
      " [1029.    0.    0.    0.]\n",
      " [1029.    0.    0.    0.]]\n",
      "[[526. 159.   0.   0.]\n",
      " [489. 196.   0.   0.]\n",
      " [511. 174.   0.   0.]\n",
      " [529. 156.   0.   0.]\n",
      " [685.   0.   0.   0.]\n",
      " [685.   0.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train test validation Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "x_train, x_test, y_train, y_test = train_test_split( data, labels, test_size=0.05, random_state=42)\n",
    "x_test, x_val, y_test, y_val = train_test_split( data, labels, test_size=0.6, random_state=42)\n",
    "print('Size of train, validation, test:', len(y_train), len(y_val), len(y_test))\n",
    "\n",
    "print('real & fake news in train,valt,test:')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_val.sum(axis=0))\n",
    "print(y_test.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "V2axjPOV3F_v",
    "outputId": "fc1e2312-7f8a-4a6a-f407-e9a5f307be37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors in Glove.\n"
     ]
    }
   ],
   "source": [
    "#Using Pre-trained word embeddings \n",
    "os.chdir(\"C:\\\\Users\\\\rohan\\\\Desktop\\\\LeadingIndia.ai\")\n",
    "GLOVE_DIR = \"data\" \n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    #print(values[1:])\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors in Glove.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 836
    },
    "colab_type": "code",
    "id": "jGktxUME3JXT",
    "outputId": "e990f9a9-2882-4ad8-fc39-9780a3687659"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the simple convolutional neural network model\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 100)          699700    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 100, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 20, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               327808    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 1,092,410\n",
      "Trainable params: 1,092,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_2 to have 2 dimensions, but got array with shape (1628, 6, 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-e918ecd2adf1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m history = model.fit(x_train, y_train, validation_data=(x_val, y_val),\n\u001b[1;32m---> 19\u001b[1;33m           epochs=10, batch_size=32)\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 621\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    622\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    133\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    136\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_2 to have 2 dimensions, but got array with shape (1628, 6, 4)"
     ]
    }
   ],
   "source": [
    "# Simple CNN model\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_cov1= Conv1D(128, 5, activation='relu',padding='same',kernel_regularizer='l2')(embedded_sequences)\n",
    "l_pool1 = MaxPooling1D(5,padding='same')(l_cov1)\n",
    "\n",
    "l_flat = Flatten()(l_pool1)\n",
    "l_dense = Dense(128, activation='relu',kernel_regularizer='l2')(l_flat)\n",
    "preds = Dense(6, activation='softmax')(l_dense)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"Fitting the simple convolutional neural network model\")\n",
    "model.summary()\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "colab_type": "code",
    "id": "cYfk5rP2IhPT",
    "outputId": "40bb25be-bbd0-4d14-90de-45250b278417"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Z9pITEhHOvdL",
    "outputId": "2c360111-ee3b-4dd6-8c38-4713f080620a"
   },
   "outputs": [],
   "source": [
    "#convolutional approach\n",
    "convs = []\n",
    "filter_sizes = [3,4,5]\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "for fsz in filter_sizes:\n",
    "    l_conv = Conv1D(nb_filter=128,filter_length=fsz,activation='relu')(embedded_sequences)\n",
    "    l_pool = MaxPooling1D(5)(l_conv)\n",
    "    convs.append(l_pool)\n",
    "    \n",
    "l_merge = Concatenate(axis=1)(convs)\n",
    "l_cov1= Conv1D(filters=128, kernel_size=5, activation='relu',padding='same')(l_merge)\n",
    "l_pool1 = MaxPooling1D(5,padding='same')(l_cov1)\n",
    "l_cov2 = Conv1D(filters=128, kernel_size=5, activation='relu',padding='same')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(30,padding='same')(l_cov2)\n",
    "l_flat = Flatten()(l_pool2)\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "preds = Dense(6, activation='softmax')(l_dense)\n",
    "\n",
    "model2 = Model(sequence_input, preds)\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"Fitting a more complex convolutional neural network model\")\n",
    "model2.summary()\n",
    "history2 = model2.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=3, batch_size=32)\n",
    "model2.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "colab_type": "code",
    "id": "ZBOpm2CbQolr",
    "outputId": "93963fbf-8cdd-4c26-f27a-153a4676dc25"
   },
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history2.history.keys())\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "# summarize history for accuracy\n",
    "plt.plot(history2.history['acc'])\n",
    "plt.plot(history2.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history2.history['loss'])\n",
    "plt.plot(history2.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "colab_type": "code",
    "id": "iK-UAx5FpZFm",
    "outputId": "3ee4d2d6-eace-438b-e1d0-bfe9d5e09845"
   },
   "outputs": [],
   "source": [
    "# Test model 1\n",
    "test_preds = model.predict(x_test)\n",
    "test_preds = np.round(test_preds)\n",
    "correct_predictions = float(sum(test_preds == y_test)[0])\n",
    "print(\"Correct predictions:\", correct_predictions)\n",
    "print(\"Total number of test examples:\", len(y_test))\n",
    "print(\"Accuracy of model1: \", correct_predictions/float(len(y_test)))\n",
    "\n",
    "# Creating the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "x_pred = model.predict(x_test)\n",
    "x_pred = np.round(x_pred)\n",
    "x_pred = x_pred.argmax(1)\n",
    "y_test_s = y_test.argmax(1)\n",
    "cm = confusion_matrix(y_test_s, x_pred)\n",
    "plt.matshow(cm, cmap=plt.cm.binary, interpolation='nearest')\n",
    "plt.title('Confusion matrix - model1')\n",
    "plt.colorbar()\n",
    "plt.ylabel('expected label')\n",
    "plt.xlabel('predicted label')\n",
    "# plt.show()\n",
    "\n",
    "#Test model 2\n",
    "test_preds2 = model2.predict(x_test)\n",
    "test_preds2 = np.round(test_preds2)\n",
    "correct_predictions = float(sum(test_preds2 == y_test)[0])\n",
    "print(\"Correct predictions:\", correct_predictions)\n",
    "print(\"Total number of test examples:\", len(y_test))\n",
    "print(\"Accuracy of model2: \", correct_predictions/float(len(y_test)))\n",
    "\n",
    "# Creating the Confusion Matrix\n",
    "x_pred = model2.predict(x_test)\n",
    "x_pred = np.round(x_pred)\n",
    "x_pred = x_pred.argmax(1)\n",
    "y_test_s = y_test.argmax(1)\n",
    "cm = confusion_matrix(y_test_s, x_pred)\n",
    "plt.matshow(cm, cmap=plt.cm.binary, interpolation='nearest',)\n",
    "plt.title('Confusion matrix - model2')\n",
    "plt.colorbar()\n",
    "plt.ylabel('expected label')\n",
    "plt.xlabel('predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "HhMOZMEjpeN4",
    "outputId": "1d03ef34-7b31-45cb-9603-1917f7fc24f7"
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "SVDcRz0VphiQ",
    "outputId": "98c765f4-9ee7-41e9-ee59-276d8c1e6e7c"
   },
   "outputs": [],
   "source": [
    "score = model2.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "CNN-LIAR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
